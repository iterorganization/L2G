#!/usr/bin/env python3
print("Starting up...")
import time

import argparse

description = """Runs a L2G case, described within a L2G JSON file.
"""

parser = argparse.ArgumentParser(description=description)

parser.add_argument('-i', '--input', type=str,
                    required=True, help='Input Json file')
parser.add_argument('-v', '--verbose', help='Verbosity',
                    default=0, type=int)

parser.add_argument('-p', '--progress', help='Progress bar. Enable/Disable return carriage',
                    default=1, type=int)
parser.add_argument('-r', '--run_flt', help='Run FLT tracing',
                    default=1, type=int)
parser.add_argument('-fl', '--get_fls', help="Obtain fieldlines",
                    default=1, type=int)
parser.add_argument('-cd', '--calculate_drsep', help="Calculate drsep",
                    default=1, type=int)
args = parser.parse_args()

RETURN_CARRIAGE = "\r\t"
if not args.progress:
    RETURN_CARRIAGE = "\n\t"


import os
import sys

# Check if the JSON file can be read.
json_input = args.input
if not os.access(json_input, mode=os.R_OK):
    print(f"Could not read: '{json_input}'!")
    sys.exit(-1)

# Now that everything went okay start with the module loads. Putting these
# module loads at the top of the file creates a time overhead.

import L2G
import L2G.utils
import L2G.meshio_utils
import medcoupling as mc
import MEDLoader as ml
from L2G.core import PyEmbreeAccell
if args.verbose:
    L2G.addStreamHandler()
    L2G.enableLogging()
import glob
import numpy as np

# Load json file
inp = L2G.utils.load_l2g_json(json_input)
# Create an instance of the FieldLineTracer so that we start setting parameters
# and input data.
case = L2G.utils.FieldLineTracer()

time_start = time.perf_counter()
print()
print('Loading case parameters.')

case.name = inp['name']
# Set the parameters. If unknown, just print and ignore
for parameter in inp['parameters']:
    if not hasattr(case.parameters, parameter):
        print(f"Illegal parameter: {parameter}. Ignored")
    else:
        setattr(case.parameters, parameter, inp['parameters'][parameter])

# Set the options
for option in inp['options']:
    if not hasattr(case.options, option):
        print(f"Illegal option: {option}. Ignored")
    else:
        setattr(case.options, option, inp['options'][option])

# See if there are any FL ids to set
fl_ids = []
if "fl_ids" in inp:
    fl_ids = inp["fl_ids"]

correct_helicity = True
if "correct_helicity" in inp:
    correct_helicity = inp["correct_helicity"]

# See if there is a custom wall limiter
if 'wall_limiter' in inp:
    custom_wall_limiter = True
    print("Custom wall limiter included")
else:
    custom_wall_limiter = False

# Let's see if we run in a HPC job. Currently only slurm based.
slurm_job = False
if "SLURM_JOBID" in os.environ:
    print('Detected SLURM environment.')
    job_id = os.environ['SLURM_JOBID']
    print(f"SLURM JOB ID: {job_id}")
    print("Setting the number of OpenMP threads equal to the number of "
          + "allocated CPUs")
    slurm_job = True

if slurm_job:
    # In this case assign the number of threads equal to the number of
    # CPUs per task.
    # We only have 1 distributed task.

    if "SLURM_CPUS_PER_TASK" in os.environ:
        print('Trying to get maximum number of cpus assigned for this task.')
        try:
            cpus_per_task = int(os.environ["SLURM_CPUS_PER_TASK"])
            case.parameters.num_of_threads = cpus_per_task
            print(f"case.parameters.num_of_threads={cpus_per_task}")
        except:
            print("Failed to obtain a number from SLURM_CPUS_PER_TASK")

TIME_LOADING_CASE = time.perf_counter() - time_start
print(f"Loaded case in {TIME_LOADING_CASE} seconds.")

print()

time_start = time.perf_counter()

# Load the target mesh
if not os.access(inp["target_mesh"], os.R_OK):
    print(f"Failed to read target mesh {inp['target_mesh']}!")
    print("Check the validity of the path.")
    sys.exit(-1)
print(f"\tLoading {inp['target_mesh']}")
verticesTarget, trianglesTarget = L2G.meshio_utils.readMesh(inp["target_mesh"])
case.setTargetData(verticesTarget, trianglesTarget)
TIME_LOADING_TARGET_MESH = time.perf_counter() - time_start
print(f"Loaded target mesh in {TIME_LOADING_TARGET_MESH}")

time_start = time.perf_counter()
# Create Embree
embree = PyEmbreeAccell()

# Use glob if it is necessary
shadowMeshFiles = []

# Gather the file names
for fileName in inp["shadow_meshes"]:
    if '*' in fileName:
        shadowMeshFiles += glob.glob(fileName)
    else:
        shadowMeshFiles.append(fileName)

# Exclude any meshes if exclude_meshes is inside
if "exclude_meshes" in inp:
    mesh_to_remove = []
    # Since files are actual paths it is the easiest to just loop the list
    # and accumulate which meshes to remove
    for filePath in shadowMeshFiles:
        fileName = os.path.basename(filePath)
        if fileName in inp["exclude_meshes"]:
            mesh_to_remove.append(filePath)
    #
    for m in set(mesh_to_remove):
        shadowMeshFiles.remove(m)

# Now load it
c = 0
N = len(shadowMeshFiles)

# List of loaded geometry. Th egeomIDs list is used to generate a text
# file so that the user can check in the results to which geometry does the
# geometry Id belongs to.
geomIds = []

# Include the target mesh into the shadow object. Usually, the first mesh
# gets the geometry ID 0 when loading to Embree.
if "include_target_in_shadow" in inp:
    if inp["include_target_in_shadow"]:
        print("\tLoading target mesh also to Embree")
        print("")
        # v, t comes from before block. Do not delete it!!!
        geomId = embree.commitMesh(
            verticesTarget * case.parameters.target_dim_mul,
            trianglesTarget)
        geomIds.append((geomId, inp["target_mesh"]))
print(f"Loading {N} meshes")

# Load the shadow meshes. v, t corresponds to vertices, triangles.
for filePath in shadowMeshFiles:
    fileName = os.path.basename(filePath)
    sys.stdout.write(f"{RETURN_CARRIAGE}Loading {fileName}: {c / N:.2f}%!")
    v, t = L2G.meshio_utils.readMesh(filePath)
    c+=100
    sys.stdout.write(f"{RETURN_CARRIAGE}Loaded {fileName}: {c / N:.2f}%!")
    geomId = embree.commitMesh(v * 1e-3, t)
    geomIds.append((geomId, filePath))
print('')
print(f"Writing list of loaded meshes to {case.name}_embree_meshes.txt")
with open(f"{case.name}_embree_meshes.txt", 'w') as f:
    for pair in geomIds:
        f.write(f'{pair[0]} {pair[1]}\n')
case.setEmbreeObj(embree)
TIME_LOADING_SHADOW_MESHES = time.perf_counter() - time_start

print(f"Loading {inp['eq_type']} equilibriums")

equilIter = L2G.utils.EquilibriumIterator()

equilIter.correctHelicity(correct_helicity)
# Figure out what kind of equilibriums do we have
if inp['eq_type'] == "imas":
    equilIter.loadIMASEquilibriums(inp['imas'])
else:
    equilIter.loadEqdskEquilibriums(inp['eqdsk_files'])

# Create the result MED file.
resultMesh = mc.ReadMeshFromFile(inp["target_mesh"])
resultFileName = os.path.join(os.path.dirname(os.path.abspath(args.input)),
                case.name + '.med')

if os.path.exists(resultFileName):
    print("Warning result file already exists! Overwriting!")
    # time.sleep(10)

ml.WriteMesh(resultFileName, resultMesh, True)


# Now doing the main thing
_c = 0 # In case of EQDSK, the associated time to write to MED field. In case
       # of multiple EQDSK G files.

# NOTE THAT THE INDEX "i" IS THE MAIN INDEX, SO SUBSEQUENT FOR LOOPS INSIDE
# MUST NOT USE "i" AS INDEX!

TIME_RUNNING_CASES = 0
n_equils = len(equilIter)
for i, associated_time, equilibrium in equilIter:
    time_start = time.perf_counter()
    print(f"Getting results for {i + 1} of {n_equils} equilibriums.")
    # Get EQDSKIO

    if custom_wall_limiter:
        equilibrium.wall_contour_r = np.array(inp['wall_limiter']['r'])
        equilibrium.wall_contour_z = np.array(inp['wall_limiter']['z'])

    case.setEquilibrium(equilibrium)

    # Apply parameters and load Equilibrium. This is done everytime
    case.applyParameters()
    case.loadEq()

    case.processDataOnMesh()
    # Running FLT and saving FLT data on input mesh
    if args.run_flt:
        print("Running FLT...")
        case.runFltOnMesh()

    if args.calculate_drsep:
        print("Calculating drsep...")
        case.calculateDrsep()

    print("Saving results to MED file...")
    case.saveMeshResultsToExistingMed(mesh_med=resultMesh,
        output_med_file=resultFileName, iteration=i,
        associated_time=associated_time)
    # Running FLT to get FLs from selected ids.
    if args.get_fls and fl_ids:
        print("Getting FLs...")
        # Obtain FLs and output them into a file.
        if all(isinstance(x, list) for x in fl_ids):
            # We have a group of IDs.
            for j, subset in enumerate(fl_ids):
                case.fl_ids = subset
                case.getFL()
                case.saveFlToVTK(f"{case.name}_{j}_{i}.vtk")

        else:
            case.fl_ids = fl_ids
            case.getFL()
            case.saveFlToVTK(f"{case.name}_{i}.vtk")

    TIME_RUNNING_CASES += time.perf_counter() - time_start

print("Time summaries:")
print("\tLoading case:".rjust(25) + f"{TIME_LOADING_CASE:.2f} s")
print("\tLoading target mesh:".rjust(25) + f"{TIME_LOADING_TARGET_MESH:.2f} s")
print("\tLoading shadow meshes:".rjust(25) + f"{TIME_LOADING_SHADOW_MESHES:.2f} s")
print("\tRunning cases:".rjust(25) + f"{TIME_RUNNING_CASES:.2f} s")
print()
print(f"Total: {TIME_LOADING_CASE + TIME_LOADING_TARGET_MESH + TIME_LOADING_SHADOW_MESHES + TIME_RUNNING_CASES:.2f}s")